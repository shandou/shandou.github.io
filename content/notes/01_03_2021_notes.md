Title: PySpark Learning Journal 01: Loading CSV From URL
Date: 2021-01-03
Category: Data Science
Tags: ds, pyspark
Slug: pyspark-notes
Authors: Shan Dou
Summary: Notes made when following Josh Starmer's tutorial



## Objective

To prepare for dataframe manipulations with pyspark, I wanted to first load the titanic dataset with pyspark. Loading csv file directly from local directory is straightforward (one-liner  `spark.read.csv("local.csv")`  gets the job done), but loading the same dataset via a GitHub URL takes a bit more work and can look foreign for pyspark beginners. This note is entered for my future self, and for anyone else who has similar questions as they start their pyspark journey üòÉ

All the code examples were run and tested in Jupyter Lab launched via Azure machine learning service.

```python
# The data path  
data_path = (  
    "https://raw.githubusercontent.com/shandou/"
    "ml_data_collection/master/classification/titanic.csv"
)
```

## Preparation: Set up Spark Session
```python
import pyspark  
from pyspark.sql import SparkSession

# Note: this is only one of the approaches to create a spark session  
spark = SparkSession.builder.getOrCreate()
```

## CSV Loading: Two Approaches

### Approach 1: (Two steps) Load with pandas first, then convert to pyspark dataframe

-   Pros: The loading step is simple thanks to  `pandas.read_csv`
-   Cons: pandas becomes an extra ‚Äúmiddle man‚Äù and has little to do other than being a file loading helper

```python
# 0. import  
import pandas as pd# 1. Load the data as pandas dataframe  
df = pd.read_csv(data_path)# 2. Convert to spark dataframe  
# [IMPORTANT] At this point, df_spark is only stored locally  
# and not yet registered into the spark session's catalog.   
# Without additonal steps, spark session methods such as ``.sql()`` will throw errors  
df_spark = spark.createDataFrame(df)
```

At the end of step2, although the Spark data loading has been achieved, it is not yet locally visible. In this case, when we run `spark.catalog.listTables()` , we will obtain an empty list.

```python
# 3. Register ``df_spark`` as a temporary table named "temp"  
df_spark.createOrReplaceTempView("temp")
```

After the registration, when we run  `spark.catalog.listTables()`, we get:

```python
[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]
```

### Approach 2: Single-step pyspark loading

```python
# [IMPORTANT] We need to first add the file path to sparkContextfrom pyspark import SparkFilesspark.sparkContext.addFile(data_path)  
df_spark = spark.read.csv(  
    "file://" + SparkFiles.get("titanic.csv"),  
    header=True, inferSchema=True  
)
```

Note that the use of `sparkContent` is necessary. If we directly feed `data_path` to `spark.read.csv`, we will get error messages that look like this:
![pyspark error](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/6158d6cb-f05a-4a2e-a438-69f4d5837138/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210103%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210103T230406Z&X-Amz-Expires=86400&X-Amz-Signature=392932fc7aefabb6ccb52ca0b3b7ddb2e83a33e2e6b091cf65be142cbbd864c6&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

## Wrap Up

Both approaches generate the same spark dataframe ready for querying.

```python
print(df_spark)
Output:  
DataFrame[Survived: int, Age: double, Fare: double, Pclass_1: int, Pclass_2: int, Pclass_3: int, Sex_female: int, Sex_male: int, SibSp_0: int, SibSp_1: int, SibSp_2: int, SibSp_3: int, SibSp_4: int, SibSp_5: int, SibSp_8: int, Parch_0: int, Parch_1: int, Parch_2: int, Parch_3: int, Parch_4: int, Parch_5: int, Parch_6: int, Embarked_C: int, Embarked_Q: int, Embarked_S: int]
```

## More Notes to be Made

Relevant notes that are in the plans:

-   Session and Context in spark
-   Data processing in pyspark

## Reference

1.  [StackOverflow thread on loading csv via URL in pyspark](https://stackoverflow.com/questions/57014043/reading-data-from-url-using-spark-databricks-platform)
2.  Datacamp‚Äôs ‚Äú[Introduction to PySpark](https://learn.datacamp.com/courses/introduction-to-pyspark)‚Äù course
