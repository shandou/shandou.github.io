Title: Notes on ensemble learnings
Date: 2021-01-06
Category: Machine Learning
Tags: ml, ensemble
Slug: ensemble-ml-notes
Authors: Shan Dou
Summary: Refresh old notes on ensemble learning



## Before we begin: Deep learnings vs. Ensemble, how to pick?
General rule of thumb to follow is to check the dimension of the features space (and of course, the number of samples we have). One interesting case in the [click fraud detection example](https://github.com/shandou/MLND_capstone/blob/master/MLNDcapstone_report_final.pdf) I had worked on back in 2018 is "large n (number of samples) but small m (number of features)". 

> Because the dimension of the feature space is small (even after the feature engineering steps described in later sections), deep learning algorithms are not considered and we focus primarily on ensemble algorithms that use decision tree classifiers as base learners.

This goes to show that both n and m needs to be sufficiently large for deep learning.

Also, as a refresh, let's keep the following heuristics in mind as well (Ref1):
* For clean data, the number of samples need to be at least 10x of the number of features
* For noisy data, this ratio could go up to 100x or even 1000x more

## What is ensemble learning?
Ensemble learning is a technique for aggregating predictions obtained from a group of base predictors. These base predictors could be trained in parallel with each predictors working on a different random subset of the training data, or they could be trained sequentially with each added learner working on improving the predictions of its predecessor.

## Different flavors of ensemble learning
### Bagging and pasting
They are the **parallel** ensembles (i.e., each base learner works on a subset of training data). For such ensembles, when the aggregated predictions are computed with a simple average across the predictions of all individual learnings, we call it **bagging** (when training data sampling is performed with replacement) or **pasting** (when training data sampling is performed without replacement) ensemble. Random forest is a classic example of the bagging ensemble (with the same base learner type in this case). It makes predictions by averaging the predictions of a group of decision trees that each works on a different random subset of the training data (sampled with raplacement).

### Boosting
Boosting ensemble refers to the technique of training a group of predictors sequentially, with each of the added predictor working on improving the predictions of its predecessor. **Adaptive boosting** and **gradient boosting** are the two major type of boosting:
* For adaptive boosting, the predictions of each precedessor are used to assign weights to the training data for its successor, and the predction aggregation is achieved by taking a weighted average over the predictions of all predictors (The better performed predictor earns a higher weight in the wweighted average)
* For gradient boosting, instead of using predecessor to tweak the weights of the training data and the predictors, each predictor is added to the sequence to fit the residual error of its predecessor.

> **_NOTE_** If you ever wonder why in xgboost we call the number of trees "the number of boosting round", think of what we are doing in boosting--we are adding ***a tree*** in ***each boosting round*** to improve the predecessor.


### Stacking
Stacking refers to the approach used to aggregate the predictions. Instead of taking a simple average like in bagging and adaptive boosting, stacking ensemble trains a meta learner (also known as a ***blender***) to perform the aggregation. In this way, stacking makes the aggregation step more objective and allows great flexibility in the structure of the ensemble. For example, one can use a multilayer stacking in which the first few meta-learner layers are several different blenders and the last meta-learner layer is a single blender tha carries out the final aggregation.

> **_CAUTION_** Blender is more objective and flexible than simple averaging or voting, but it comes with the price that we must split the data further--not only to train individual predictors, but also to train the blender itself. When data size is limited, simple aggregation would do the trick just fine :)


## Good libraries
| Algorithm                 | Ensemble type     | Library implementations                           |
|---------------------------|-------------------|---------------------------------------------------|
| Random forest             | Bagging           | sklearn.ensemble.RandomForestClassifier/Regressor |
| Stacking                  | Stacking          | mlens.ensemble.SuperLearner                       |
| Extreme gradient boosting | Gradient boosting | xgboost.XGBClassifier/Regressor                   |
| Light gradient boosting   | Gradient boosting | lightgbm.LGBMClassifier/Regressor                 |


## References
1. StackExchange thread ["Minimum Training size for simple neural net"](https://stats.stackexchange.com/questions/257292/minimum-training-size-for-simple-neural-net)
2. Aurélien Géron, Sep 2019, "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition", O'Reilly Media, Inc.
