<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>Shan's Second Brain - cross-validation</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Shan's Second Brain</a></h1>
                <nav><ul>
                    <li><a href="/pages/About.html">About</a></li>
                    <li><a href="/category/data-science.html">Data Science</a></li>
                    <li><a href="/category/machine-learning.html">Machine Learning</a></li>
                    <li><a href="/category/reading-list.html">Reading List</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/notes/cross-validation-ml-notes.html">The What and How on Cross-Validation for Supervised Learning</a></h1>
<footer class="post-info">
        <abbr class="published" title="2021-02-06T00:00:00-08:00">
                Published: Sat 06 February 2021
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/shan-dou.html">Shan Dou</a>
        </address>
<p>In <a href="/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="/tag/ml.html">ml</a> <a href="/tag/cross-validation.html">cross-validation</a> </p>
</footer><!-- /.post-info --><h2>The Dichotomy Between Test and Validation</h2>
<h3>1. Metamorphic explainer for visceral mental imageries</h3>
<p>Given the powerful effect of association and mental imagery, we start this note with an analogy: If we think of supervised learning tasks as a stage play or a high-stake exam, validations are the dress rehearsals or the mock exams we carry out to (1) test our strategy/knowledge in a safe environment and (2) form feedback loops for improving our strategy/knowledge based on the validation outcome. Similarly, we can view tests as the “moment-of-truth” real contest or actual stage play.</p>
<h3>2. More rigorous explainer</h3>
<p>In more formal language, validation is part of the training data that are partitioned out as a proxy of test datasets to give us some insight into the predictive capability of the model. Put it more concretely, validation serves two main purposes under the umbrella of model selection:
1. Select among different models based on validation scores
2. Select optimal combinations of hyperparameters for each given model
In practice, these two activities are carried out in parallel in scikit learn classes GridSearchCV and RandomizedSearchCV .</p>
<h2>What Is the “Cross” for in Cross-Validation?</h2>
<p>“Cross” has two layers of meaning:
1. Signifies the use of sampling without replacement so that each training example is used in training and validation exactly once. The opposite of this approach — sampling with replacement — can be compared to the ineffective learning strategy of rereading repeatedly and being fooled by the illusion of knowing.
2. Circumvents the risk of fitting to the model to a particular way of partition the train vs. validation sets. For example, for a 5-fold cross-validation, if we simply do the partition and arbitrarily decide to only carry out validation once with the first chunk of the dataset as the validation, we could get unlucky that some systematic biases happen to be present in such partitioning and the model ends up being distracted by such noises.</p>
<h2>Why Are Validation and Test Both Necessary?</h2>
<p>During validation, we expose the model to all the examples in the training data. The risk here is that we cultivate a sense of familiarity for the model that is easy to confuse rote memorization with actual learning. And also like real-life situations when we could think that we had mastered the test materials but ended up doing poorly in the real exams, the test is necessary to — well, really put the model’s learning into tests.</p>
<h2>Best Practices</h2>
<p>The following guidelines can be found in Raschka and Mirjalili (2019):
1. For data sizes that are not too large or too small, a good standard value for k in k-fold cross-validation is 10. <mark>10-fold CV</mark> is suggested to achieve the best tradeoff between bias and variance
2. A general rule of thumb is: We <mark>increase</mark> the k values in k-fold CV when the data size is <mark>small</mark>. One core reason behind this rule is to maximize the size of training data in each CV iteration. Otherwise, because the data size is already very small to start with, we risk having “pessimistic bias” when estimating model performance. One extreme approach for very small training data is the “leave-one-out cross-validation” (LOOCV); In LOOCV, the number of folds equals the total number of training examples. In each CV iteration, only one training example is treated as validation data.
3. By contrast, when we work with a <mark>large</mark> dataset, the concern shifts toward <mark>computation costs and agility in parameter adjustment</mark>. This is when we want to decrease k values (e.g., k = 5 could be a good start)</p>
<h2>Closing Words</h2>
<p>Things that seem trivial may not easily be articulated. Same as the “putting our learning to tests” argument, articulating and forming associations and intuitions around core (and seemingly trivial) concepts are a crucial part of mastery.</p>
<h2>References</h2>
<ol>
<li>Browlee, J., 2017, <a href="https://machinelearningmastery.com/difference-test-validation-datasets/">“What is the Difference Between Test and Validation Datasets?” blog post</a>.</li>
<li>Raschka, S. and Mirjalili, V., 2019, “Learning Best Practices for Model Evaluation and Hyperparameter Tuning”, <a href="https://www.packtpub.com/product/python-machine-learning-third-edition/9781789955750">Python Machine Learning — Third Edition</a>.</li>
<li>Brown, P.C., Roediger H.L., and McDaniel, 2014, “Avoid Illusions of Knowing”, <a href="https://www.goodreads.com/book/show/18770267-make-it-stick">Make It Stick: The Science of Successful Learning</a>.</li>
</ol>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>